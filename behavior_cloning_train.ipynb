{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x1f89cd33310>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import all our dependencies\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from skimage import io, transform\n",
    "from skimage.transform import rotate\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtonyxin\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\yyton\\Documents\\GitHub\\Launchkart\\wandb\\run-20220312_121143-jr9h2qid</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/launchkart/tony-behavior-cloning-training/runs/jr9h2qid\" target=\"_blank\">worthy-breeze-1</a></strong> to <a href=\"https://wandb.ai/launchkart/tony-behavior-cloning-training\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize weight and biases trial so we can monitor our trials\n",
    "#Make sure to rerun this cell before every trial!\n",
    "wandb.init(project=\"tony-behavior-cloning-training\", entity=\"launchkart\")\n",
    "#TODO: Name this something descriptive, and make sure to rename the trial name for different trials!\n",
    "wandb.run.name = \"tony-trial-1\"\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#Create our torch device, which will be cuda if GPU and cpu if just using CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(385, 66, 200, 3)\n",
      "(385, 5)\n"
     ]
    }
   ],
   "source": [
    "x = np.load(\"data/X.npy\")\n",
    "y = np.load(\"data/y.npy\")\n",
    "\n",
    "#If you are loading more than one npy file, you will need to load all of them and then use vstack to concatenate them all together \n",
    "# x = np.vstack(x, ...)\n",
    "# y = np.vstack(y, ...)\n",
    "\n",
    "#Split our training data into a training set and validation set\n",
    "split_idx = int(0.8 * len(x))\n",
    "x_train, x_val = x[:split_idx], x[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "#Look at our training set shapes\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch has a Datasets class that makes it easy to interface with datasets when training\n",
    "#This is mostly filled in for you, although we will add data augmentation later\n",
    "class MarioKartDataset(Dataset):\n",
    "    \"\"\"Nose Keypoints dataset.\"\"\"\n",
    "    \n",
    "    #Create a list of samples, where each sample is a tensor with an observation (image) and an action (vector of controller input)\n",
    "    def __init__(self, x, y):\n",
    "        self.samples = []\n",
    "        for i in range(len(x)):\n",
    "            x_sample, y_sample = x[i], y[i]\n",
    "            sample = {'obs': x_sample, 'action': y_sample}\n",
    "\n",
    "            self.samples.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    #Gets the item at index idx from our samples\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        #Ignore this todo until we've covered it in project meeting: \n",
    "        #TODO: Apply data augmentation here\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n"
     ]
    }
   ],
   "source": [
    "#Create a training and validation dataset\n",
    "mario_kart_train_dataset = MarioKartDataset(x_train, y_train)\n",
    "mario_kart_val_dataset = MarioKartDataset(x_val, y_val)\n",
    "print(len(mario_kart_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch uses DataLoaders to handle shuffling datasets and loading batches of data\n",
    "#You can experiment with different batch sizes here\n",
    "#Num_workers can be set to 1 if using GPU\n",
    "batch_size=64\n",
    "mario_kart_train_dataloader = DataLoader(mario_kart_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "mario_kart_val_dataloader = DataLoader(mario_kart_val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Define your agent architecture here\n",
    "#Take a look at our MNIST tutorial for reference.\n",
    "#Recommended architecture is covered in meeting slides\n",
    "#Note: The in dimension for the first fully connected layer is very hard to calculate, can prob just run first and determine based on the error message\n",
    "#Note: Use Sequential to keep your code organized\n",
    "class MarioKartBCAgent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MarioKartBCAgent, self).__init__()\n",
    "        \n",
    "        #Feature Extraction Module: Conv layers (check out Conv2d, BatchNorm, Relu, Maxpool)\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=24, kernel_size=5, stride=1, padding=2), nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels=24, out_channels=36, kernel_size=5, stride=1, padding=2), nn.ReLU())\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(in_channels=36, out_channels=48, kernel_size=5, stride=1, padding=2), nn.ReLU())\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(in_channels=48, out_channels=64, kernel_size=3, stride=1, padding=1), nn.ReLU())\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        #Inference Module: Fully connected layers (check out Linear, Dropout, Relu)\n",
    "        self.fc1 = nn.Sequential(nn.Linear(211200, 100), nn.Dropout(0.2))\n",
    "        self.fc2 = nn.Sequential(nn.Linear(100, 50), nn.Dropout(0.2))\n",
    "        self.fc3 = nn.Sequential(nn.Linear(50, 10), nn.Dropout(0.2))\n",
    "        self.fc4 = nn.Sequential(nn.Linear(10, 5))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Pass x through conv layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        #Flatten x to prepare for passing into linear\n",
    "        x = torch.flatten(x, 1) \n",
    "        \n",
    "        #Pass x through linear layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        #Name variable output on last layer\n",
    "        output = x\n",
    "        \n",
    "        return output    # return x for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 , training loss: 1085.3543853759766\n",
      "Epoch  1 , validation_loss:  1299.8576049804688\n",
      "Finished Epoch 2 , training loss: 977.1715785435268\n",
      "Epoch  2 , validation_loss:  1009.7860717773438\n",
      "Finished Epoch 3 , training loss: 946.4096494402204\n",
      "Epoch  3 , validation_loss:  884.3128356933594\n",
      "Finished Epoch 4 , training loss: 920.548713684082\n",
      "Epoch  4 , validation_loss:  1096.4378967285156\n",
      "Finished Epoch 5 , training loss: 1004.1973920549665\n",
      "Epoch  5 , validation_loss:  1065.2937622070312\n",
      "Finished Epoch 6 , training loss: 921.2420817783901\n",
      "Epoch  6 , validation_loss:  1155.7324523925781\n",
      "Finished Epoch 7 , training loss: 944.1373040335519\n",
      "Epoch  7 , validation_loss:  1104.9945678710938\n",
      "Finished Epoch 8 , training loss: 941.8939377920968\n",
      "Epoch  8 , validation_loss:  1024.2730102539062\n",
      "Finished Epoch 9 , training loss: 946.2451531546457\n",
      "Epoch  9 , validation_loss:  1061.5072021484375\n",
      "Finished Epoch 10 , training loss: 932.2612947736468\n",
      "Epoch  10 , validation_loss:  948.0646362304688\n",
      "Finished Epoch 11 , training loss: 1881.302001953125\n",
      "Epoch  11 , validation_loss:  793.1477813720703\n",
      "Finished Epoch 12 , training loss: 2011.583243233817\n",
      "Epoch  12 , validation_loss:  1043.81201171875\n",
      "Finished Epoch 13 , training loss: 1075.609584263393\n",
      "Epoch  13 , validation_loss:  848.1456604003906\n",
      "Finished Epoch 14 , training loss: 948.7239783150809\n",
      "Epoch  14 , validation_loss:  1043.7016296386719\n",
      "Finished Epoch 15 , training loss: 976.0541390010288\n",
      "Epoch  15 , validation_loss:  1139.6570739746094\n",
      "Finished Epoch 16 , training loss: 910.3745275224958\n",
      "Epoch  16 , validation_loss:  958.067138671875\n",
      "Finished Epoch 17 , training loss: 902.9466718946185\n",
      "Epoch  17 , validation_loss:  941.8641967773438\n",
      "Finished Epoch 18 , training loss: 913.4650241647448\n",
      "Epoch  18 , validation_loss:  885.0197143554688\n",
      "Finished Epoch 19 , training loss: 928.8703280857632\n",
      "Epoch  19 , validation_loss:  1055.2906188964844\n",
      "Finished Epoch 20 , training loss: 917.6529867989676\n",
      "Epoch  20 , validation_loss:  941.253662109375\n",
      "Finished Epoch 21 , training loss: 987.6707371303013\n",
      "Epoch  21 , validation_loss:  971.0018920898438\n",
      "Finished Epoch 22 , training loss: 923.482155936105\n",
      "Epoch  22 , validation_loss:  1026.62890625\n",
      "Finished Epoch 23 , training loss: 927.2835675648281\n",
      "Epoch  23 , validation_loss:  1003.1810302734375\n",
      "Finished Epoch 24 , training loss: 898.4929466247559\n",
      "Epoch  24 , validation_loss:  1089.3819580078125\n",
      "Finished Epoch 25 , training loss: 886.4100666046143\n",
      "Epoch  25 , validation_loss:  900.4168701171875\n",
      "Finished Epoch 26 , training loss: 965.8426077706473\n",
      "Epoch  26 , validation_loss:  1008.6979370117188\n",
      "Finished Epoch 27 , training loss: 939.8055441720145\n",
      "Epoch  27 , validation_loss:  1150.150634765625\n",
      "Finished Epoch 28 , training loss: 939.239519391741\n",
      "Epoch  28 , validation_loss:  1042.6728515625\n",
      "Finished Epoch 29 , training loss: 913.7724601200649\n",
      "Epoch  29 , validation_loss:  981.6911926269531\n",
      "Finished Epoch 30 , training loss: 913.9850834437779\n",
      "Epoch  30 , validation_loss:  1022.7462768554688\n",
      "Finished Epoch 31 , training loss: 906.2373897007534\n",
      "Epoch  31 , validation_loss:  1037.2900390625\n",
      "Finished Epoch 32 , training loss: 940.8208705357143\n",
      "Epoch  32 , validation_loss:  972.0455627441406\n",
      "Finished Epoch 33 , training loss: 925.2616691589355\n",
      "Epoch  33 , validation_loss:  984.9088745117188\n",
      "Finished Epoch 34 , training loss: 936.6329084123884\n",
      "Epoch  34 , validation_loss:  815.1955337524414\n",
      "Finished Epoch 35 , training loss: 908.7839911324637\n",
      "Epoch  35 , validation_loss:  1030.7301025390625\n",
      "Finished Epoch 36 , training loss: 1015.8409423828125\n",
      "Epoch  36 , validation_loss:  915.7931823730469\n",
      "Finished Epoch 37 , training loss: 922.5037362234933\n",
      "Epoch  37 , validation_loss:  1087.7235107421875\n",
      "Finished Epoch 38 , training loss: 938.8862675258091\n",
      "Epoch  38 , validation_loss:  1020.8056335449219\n",
      "Finished Epoch 39 , training loss: 930.3245827811105\n",
      "Epoch  39 , validation_loss:  1043.6751403808594\n",
      "Finished Epoch 40 , training loss: 928.0707332066128\n",
      "Epoch  40 , validation_loss:  919.3688659667969\n",
      "Finished Epoch 41 , training loss: 911.0699375697544\n",
      "Epoch  41 , validation_loss:  979.470947265625\n",
      "Finished Epoch 42 , training loss: 929.5798863002232\n",
      "Epoch  42 , validation_loss:  853.3653259277344\n",
      "Finished Epoch 43 , training loss: 888.9509721483503\n",
      "Epoch  43 , validation_loss:  1068.1571044921875\n",
      "Finished Epoch 44 , training loss: 944.3961377825055\n",
      "Epoch  44 , validation_loss:  956.0614318847656\n",
      "Finished Epoch 45 , training loss: 938.3141392299107\n",
      "Epoch  45 , validation_loss:  986.4378356933594\n",
      "Finished Epoch 46 , training loss: 889.5556651524136\n",
      "Epoch  46 , validation_loss:  918.8185424804688\n",
      "Finished Epoch 47 , training loss: 947.3732147216797\n",
      "Epoch  47 , validation_loss:  1072.0922546386719\n",
      "Finished Epoch 48 , training loss: 997.3948974609375\n",
      "Epoch  48 , validation_loss:  1191.6795043945312\n",
      "Finished Epoch 49 , training loss: 919.3006853376116\n",
      "Epoch  49 , validation_loss:  1102.3930053710938\n",
      "Finished Epoch 50 , training loss: 931.0224445887974\n",
      "Epoch  50 , validation_loss:  1143.57470703125\n"
     ]
    }
   ],
   "source": [
    "#This is the cell where we define our training loop\n",
    "#It is mostly implemented already, but make sure to read through all of it to understand what is happening\n",
    "\n",
    "agent = MarioKartBCAgent()\n",
    "\n",
    "#Change the trial_name to something descriptive, based on what data you are using for training\n",
    "trial_name = 'kevin_luigicircuit_timetrials/'\n",
    "save_path = 'saved_agents/'\n",
    "\n",
    "\n",
    "cwd = os.getcwd()\n",
    "agent_dir = os.path.join(cwd,save_path)\n",
    "trial_dir = os.path.join(agent_dir, trial_name)\n",
    "if not os.path.exists(agent_dir):\n",
    "    os.mkdir(agent_dir)\n",
    "if not os.path.exists(trial_dir):\n",
    "    os.mkdir(trial_dir)\n",
    "    \n",
    "#Change the checkpoint number if you are loading a model from a checkpoint for further training\n",
    "checkpoint = 0\n",
    "# agent.load_state_dict(torch.load(save_path + trial_name + str(checkpoint)))\n",
    "\n",
    "#Define your learning rate and number of epochs to train for\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "#Initialize your optimizer and loss\n",
    "optimizer = optim.Adam(agent.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#This config is for WandB to track the hyperparameters we used for this run\n",
    "wandb.config = {\n",
    "  \"learning_rate\": lr,\n",
    "  \"checkpoint\": checkpoint,\n",
    "  \"trial_name\": trial_name,\n",
    "  \"epochs\": epochs,\n",
    "  \"batch_size\": batch_size\n",
    "}\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "#Iterate through number of epochs\n",
    "for epoch in range(epochs):\n",
    "    epoch_training_losses = []\n",
    "    epoch_validation_losses = []\n",
    "    \n",
    "    #Iterate through our dataloader batch by batch\n",
    "    for _, sample_batched in enumerate(mario_kart_train_dataloader):\n",
    "        \n",
    "        #Creates an observation and action batch\n",
    "        obs_batch, action_batch = np.transpose(sample_batched['obs'], (0, 3, 1, 2)), sample_batched['action']\n",
    "        obs_batch, action_batch = obs_batch.float().to(device), action_batch.float().to(device)\n",
    "        \n",
    "        #Optimizer performs a parameter update step based on loss calculated between predicted and grond truth actions\n",
    "        optimizer.zero_grad() \n",
    "        pred_action = agent(obs_batch)\n",
    "        loss = criterion(pred_action, action_batch.reshape(pred_action.shape))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_training_losses.append(loss.item())\n",
    "    \n",
    "    #Keep track of training losses every epoch and log with WandB\n",
    "    training_loss = np.mean(epoch_training_losses)\n",
    "    print(\"Finished Epoch\", epoch + 1, \", training loss:\", training_loss)\n",
    "    training_losses.append(training_loss)\n",
    "    wandb.log({\"training_loss\": training_loss})\n",
    "\n",
    "    #After every epoch, test agent performance on validation set, which it has not trained on\n",
    "    #This is to make sure we aren't overfitting too hard, and are generalizing well\n",
    "    with torch.no_grad():\n",
    "        #Set agent to evaluation mode so we aren't calculating gradients\n",
    "        agent.eval()\n",
    "        \n",
    "        #Iterate through our validation dataloder batch by batch\n",
    "        for _, sample_batched in enumerate(mario_kart_val_dataloader):\n",
    "            obs_batch, action_batch = np.transpose(sample_batched['obs'], (0, 3, 1, 2)), sample_batched['action']\n",
    "            obs_batch, action_batch = obs_batch.float().to(device), action_batch.float().to(device)\n",
    "            pred_action = agent(obs_batch)\n",
    "            val_loss = criterion(pred_action, action_batch.reshape(pred_action.shape))\n",
    "            epoch_validation_losses.append(val_loss.item())\n",
    "        \n",
    "        #Keep track of validationlosses every epoch and log with WandB\n",
    "        validation_loss = np.mean(epoch_validation_losses)\n",
    "        print(\"Epoch \", epoch +1, \", validation_loss: \", validation_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "        wandb.log({\"validation_loss\": validation_loss})\n",
    "        agent.train()\n",
    "    \n",
    "    #Save agent checkpoint every 10 epochs\n",
    "    if epoch == 0 or epoch % 10 == 9:\n",
    "        torch.save(agent.state_dict(), save_path + trial_name + str(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
