{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all our dependencies\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from skimage import io, transform\n",
    "from skimage.transform import rotate\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize weight and biases trial so we can monitor our trials\n",
    "#Make sure to rerun this cell before every trial!\n",
    "wandb.init(project=\"kevin-behavior-cloning-training\", entity=\"launchkart\")\n",
    "#TODO: Name this something descriptive, and make sure to rename the trial name for different trials!\n",
    "wandb.run.name = \"kevin-trial-1\"\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our torch device, which will be cuda if GPU and cpu if just using CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"data/X.npy\")\n",
    "y = np.load(\"data/y.npy\")\n",
    "\n",
    "#If you are loading more than one npy file, you will need to load all of them and then use vstack to concatenate them all together \n",
    "# x = np.vstack(x, ...)\n",
    "# y = np.vstack(y, ...)\n",
    "\n",
    "#Split our training data into a training set and validation set\n",
    "split_idx = int(0.8 * len(x))\n",
    "x_train, x_val = x[:split_idx], x[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "#Look at our training set shapes\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch has a Datasets class that makes it easy to interface with datasets when training\n",
    "#This is mostly filled in for you, although we will add data augmentation later\n",
    "class MarioKartDataset(Dataset):\n",
    "    \"\"\"Nose Keypoints dataset.\"\"\"\n",
    "    \n",
    "    #Create a list of samples, where each sample is a tensor with an observation (image) and an action (vector of controller input)\n",
    "    def __init__(self, x, y):\n",
    "        self.samples = []\n",
    "        for i in range(len(x)):\n",
    "            x_sample, y_sample = x[i], y[i]\n",
    "            sample = {'obs': x_sample, 'action': y_sample}\n",
    "\n",
    "            self.samples.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    #Gets the item at index idx from our samples\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        #Ignore this todo until we've covered it in project meeting: \n",
    "        #TODO: Apply data augmentation here\n",
    "        \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a training and validation dataset\n",
    "mario_kart_train_dataset = MarioKartDataset(x_train, y_train)\n",
    "mario_kart_val_dataset = MarioKartDataset(x_val, y_val)\n",
    "print(len(mario_kart_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch uses DataLoaders to handle shuffling datasets and loading batches of data\n",
    "#You can experiment with different batch sizes here\n",
    "#Num_workers can be set to 1 if using GPU\n",
    "batch_size=64\n",
    "mario_kart_train_dataloader = DataLoader(mario_kart_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "mario_kart_val_dataloader = DataLoader(mario_kart_val_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Define your agent architecture here\n",
    "#Take a look at our MNIST tutorial for reference.\n",
    "#Recommended architecture is covered in meeting slides\n",
    "#Note: The in dimension for the first fully connected layer is very hard to calculate, can prob just run first and determine based on the error message\n",
    "#Note: Use Sequential to keep your code organized\n",
    "class MarioKartBCAgent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MarioKartBCAgent, self).__init__()\n",
    "        \n",
    "        #Feature Extraction Module: Conv layers (check out Conv2d, BatchNorm, Relu, Maxpool)\n",
    "        ...\n",
    "        #Inference Module: Fully connected layers (check out Linear, Dropout, Relu)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Pass x through conv layers\n",
    "        x = ...\n",
    "        \n",
    "        #Flatten x to prepare for passing into linear\n",
    "        x = torch.flatten(x, 1) \n",
    "        \n",
    "        #Pass x through linear layers\n",
    "        x = ...\n",
    "        \n",
    "        #Name variable output on last layer\n",
    "        output = ...\n",
    "        \n",
    "        return output    # return x for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the cell where we define our training loop\n",
    "#It is mostly implemented already, but make sure to read through all of it to understand what is happening\n",
    "\n",
    "agent = MarioKartBCAgent()\n",
    "\n",
    "#Change the trial_name to something descriptive, based on what data you are using for training\n",
    "trial_name = 'kevin_luigicircuit_timetrials/'\n",
    "save_path = 'saved_agents/'\n",
    "\n",
    "\n",
    "cwd = os.getcwd()\n",
    "agent_dir = os.path.join(cwd,save_path)\n",
    "trial_dir = os.path.join(agent_dir, trial_name)\n",
    "if not os.path.exists(agent_dir):\n",
    "    os.mkdir(agent_dir)\n",
    "if not os.path.exists(trial_dir):\n",
    "    os.mkdir(trial_dir)\n",
    "    \n",
    "#Change the checkpoint number if you are loading a model from a checkpoint for further training\n",
    "checkpoint = 0\n",
    "# agent.load_state_dict(torch.load(save_path + trial_name + str(checkpoint)))\n",
    "\n",
    "#Define your learning rate and number of epochs to train for\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "#Initialize your optimizer and loss\n",
    "optimizer = ...\n",
    "criterion = ...\n",
    "\n",
    "#This config is for WandB to track the hyperparameters we used for this run\n",
    "wandb.config = {\n",
    "  \"learning_rate\": lr,\n",
    "  \"checkpoint\": checkpoint,\n",
    "  \"trial_name\": trial_name,\n",
    "  \"epochs\": epochs,\n",
    "  \"batch_size\": batch_size\n",
    "}\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "#Iterate through number of epochs\n",
    "for epoch in range(epochs):\n",
    "    epoch_training_losses = []\n",
    "    epoch_validation_losses = []\n",
    "    \n",
    "    #Iterate through our dataloader batch by batch\n",
    "    for _, sample_batched in enumerate(mario_kart_train_dataloader):\n",
    "        \n",
    "        #Creates an observation and action batch\n",
    "        obs_batch, action_batch = np.transpose(sample_batched['obs'], (0, 3, 1, 2)), sample_batched['action']\n",
    "        obs_batch, action_batch = obs_batch.float().to(device), action_batch.float().to(device)\n",
    "        \n",
    "        #Optimizer performs a parameter update step based on loss calculated between predicted and grond truth actions\n",
    "        optimizer.zero_grad() \n",
    "        pred_action = agent(obs_batch)\n",
    "        loss = criterion(pred_action, action_batch.reshape(pred_action.shape))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_training_losses.append(loss.item())\n",
    "    \n",
    "    #Keep track of training losses every epoch and log with WandB\n",
    "    training_loss = np.mean(epoch_training_losses)\n",
    "    print(\"Finished Epoch\", epoch + 1, \", training loss:\", training_loss)\n",
    "    training_losses.append(training_loss)\n",
    "    wandb.log({\"training_loss\": training_loss})\n",
    "\n",
    "    #After every epoch, test agent performance on validation set, which it has not trained on\n",
    "    #This is to make sure we aren't overfitting too hard, and are generalizing well\n",
    "    with torch.no_grad():\n",
    "        #Set agent to evaluation mode so we aren't calculating gradients\n",
    "        agent.eval()\n",
    "        \n",
    "        #Iterate through our validation dataloder batch by batch\n",
    "        for _, sample_batched in enumerate(mario_kart_val_dataloader):\n",
    "            obs_batch, action_batch = np.transpose(sample_batched['obs'], (0, 3, 1, 2)), sample_batched['action']\n",
    "            obs_batch, action_batch = obs_batch.float().to(device), action_batch.float().to(device)\n",
    "            pred_action = agent(obs_batch)\n",
    "            val_loss = criterion(pred_action, action_batch.reshape(pred_action.shape))\n",
    "            epoch_validation_losses.append(val_loss.item())\n",
    "        \n",
    "        #Keep track of validationlosses every epoch and log with WandB\n",
    "        validation_loss = np.mean(epoch_validation_losses)\n",
    "        print(\"Epoch \", epoch +1, \", validation_loss: \", validation_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "        wandb.log({\"validation_loss\": validation_loss})\n",
    "        agent.train()\n",
    "    \n",
    "    #Save agent checkpoint every 10 epochs\n",
    "    if epoch == 0 || epoch % 10 == 9:\n",
    "        torch.save(agent.state_dict(), save_path + trial_name + str(epoch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
